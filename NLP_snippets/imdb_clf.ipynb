{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "HWW7dg7i3itT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.9.0'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fetch the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 19:11:32.464549: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45267ed622b44079ab03ae23e0009772"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88e7f3e4666e4f9697c1b60715945cd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data, info \u001B[38;5;241m=\u001B[39m \u001B[43mtfds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimdb_reviews\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwith_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_supervised\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/core/logging/__init__.py:169\u001B[0m, in \u001B[0;36m_FunctionDecorator.__call__\u001B[0;34m(self, function, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    167\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_call()\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 169\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    171\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/core/load.py:640\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001B[39;00m\n\u001B[1;32m    522\u001B[0m \n\u001B[1;32m    523\u001B[0m \u001B[38;5;124;03m`tfds.load` is a convenience method that:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001B[39;00m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    634\u001B[0m dbuilder \u001B[38;5;241m=\u001B[39m _fetch_builder(\n\u001B[1;32m    635\u001B[0m     name,\n\u001B[1;32m    636\u001B[0m     data_dir,\n\u001B[1;32m    637\u001B[0m     builder_kwargs,\n\u001B[1;32m    638\u001B[0m     try_gcs,\n\u001B[1;32m    639\u001B[0m )\n\u001B[0;32m--> 640\u001B[0m \u001B[43m_download_and_prepare_builder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdbuilder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    642\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m as_dataset_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    643\u001B[0m   as_dataset_kwargs \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/core/load.py:499\u001B[0m, in \u001B[0;36m_download_and_prepare_builder\u001B[0;34m(dbuilder, download, download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[1;32m    498\u001B[0m   download_and_prepare_kwargs \u001B[38;5;241m=\u001B[39m download_and_prepare_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[0;32m--> 499\u001B[0m   \u001B[43mdbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/core/logging/__init__.py:169\u001B[0m, in \u001B[0;36m_FunctionDecorator.__call__\u001B[0;34m(self, function, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    167\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_call()\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 169\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    171\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/core/dataset_builder.py:646\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, download_dir, download_config, file_format)\u001B[0m\n\u001B[1;32m    644\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mread_from_directory(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_dir)\n\u001B[1;32m    645\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 646\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    647\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    648\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m   \u001B[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001B[39;00m\n\u001B[1;32m    652\u001B[0m   \u001B[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001B[39;00m\n\u001B[1;32m    653\u001B[0m   \u001B[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001B[39;00m\n\u001B[1;32m    654\u001B[0m   \u001B[38;5;66;03m# when reading from package data.\u001B[39;00m\n\u001B[1;32m    655\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdownload_size \u001B[38;5;241m=\u001B[39m dl_manager\u001B[38;5;241m.\u001B[39mdownloaded_size\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/core/dataset_builder.py:1498\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, download_config)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1497\u001B[0m   optional_pipeline_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 1498\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_split_generators\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001B[39;49;00m\n\u001B[1;32m   1499\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptional_pipeline_kwargs\u001B[49m\n\u001B[1;32m   1500\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m split_builder\u001B[38;5;241m.\u001B[39mnormalize_legacy_split_generators(\n\u001B[1;32m   1505\u001B[0m     split_generators\u001B[38;5;241m=\u001B[39msplit_generators,\n\u001B[1;32m   1506\u001B[0m     generator_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_examples,\n\u001B[1;32m   1507\u001B[0m     is_beam\u001B[38;5;241m=\u001B[39m\u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, BeamBasedBuilder),\n\u001B[1;32m   1508\u001B[0m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/datasets/imdb_reviews/imdb_reviews_dataset_builder.py:111\u001B[0m, in \u001B[0;36mBuilder._split_generators\u001B[0;34m(self, dl_manager)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_split_generators\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager):\n\u001B[0;32m--> 111\u001B[0m   arch_path \u001B[38;5;241m=\u001B[39m \u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_DOWNLOAD_URL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m   archive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m: dl_manager\u001B[38;5;241m.\u001B[39miter_archive(arch_path)\n\u001B[1;32m    114\u001B[0m   \u001B[38;5;66;03m# Generate vocabulary from training data if SubwordTextEncoder configured\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/core/download/download_manager.py:600\u001B[0m, in \u001B[0;36mDownloadManager.download\u001B[0;34m(self, url_or_urls)\u001B[0m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;66;03m# Add progress bar to follow the download state\u001B[39;00m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_downloader\u001B[38;5;241m.\u001B[39mtqdm():\n\u001B[0;32m--> 600\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_map_promise\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl_or_urls\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/core/download/download_manager.py:830\u001B[0m, in \u001B[0;36m_map_promise\u001B[0;34m(map_fn, all_inputs)\u001B[0m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001B[39;00m\n\u001B[1;32m    827\u001B[0m all_promises \u001B[38;5;241m=\u001B[39m tree_utils\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m    828\u001B[0m     map_fn, all_inputs\n\u001B[1;32m    829\u001B[0m )  \u001B[38;5;66;03m# Apply the function\u001B[39;00m\n\u001B[0;32m--> 830\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mtree_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_promises\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Wait promises\u001B[39;00m\n\u001B[1;32m    833\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tree/__init__.py:435\u001B[0m, in \u001B[0;36mmap_structure\u001B[0;34m(func, *structures, **kwargs)\u001B[0m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m other \u001B[38;5;129;01min\u001B[39;00m structures[\u001B[38;5;241m1\u001B[39m:]:\n\u001B[1;32m    433\u001B[0m   assert_same_structure(structures[\u001B[38;5;241m0\u001B[39m], other, check_types\u001B[38;5;241m=\u001B[39mcheck_types)\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m unflatten_as(structures[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m--> 435\u001B[0m                     [func(\u001B[38;5;241m*\u001B[39margs) \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mmap\u001B[39m(flatten, structures))])\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tree/__init__.py:435\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m other \u001B[38;5;129;01min\u001B[39;00m structures[\u001B[38;5;241m1\u001B[39m:]:\n\u001B[1;32m    433\u001B[0m   assert_same_structure(structures[\u001B[38;5;241m0\u001B[39m], other, check_types\u001B[38;5;241m=\u001B[39mcheck_types)\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m unflatten_as(structures[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m--> 435\u001B[0m                     [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mmap\u001B[39m(flatten, structures))])\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow_datasets/core/download/download_manager.py:831\u001B[0m, in \u001B[0;36m_map_promise.<locals>.<lambda>\u001B[0;34m(p)\u001B[0m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001B[39;00m\n\u001B[1;32m    827\u001B[0m all_promises \u001B[38;5;241m=\u001B[39m tree_utils\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m    828\u001B[0m     map_fn, all_inputs\n\u001B[1;32m    829\u001B[0m )  \u001B[38;5;66;03m# Apply the function\u001B[39;00m\n\u001B[1;32m    830\u001B[0m res \u001B[38;5;241m=\u001B[39m tree_utils\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[0;32m--> 831\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m p: \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, all_promises\n\u001B[1;32m    832\u001B[0m )  \u001B[38;5;66;03m# Wait promises\u001B[39;00m\n\u001B[1;32m    833\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/promise/promise.py:511\u001B[0m, in \u001B[0;36mPromise.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    509\u001B[0m     \u001B[38;5;66;03m# type: (Optional[float]) -> T\u001B[39;00m\n\u001B[1;32m    510\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_target()\n\u001B[0;32m--> 511\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mDEFAULT_TIMEOUT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_target_settled_value(_raise\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/promise/promise.py:506\u001B[0m, in \u001B[0;36mPromise._wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;66;03m# type: (Optional[float]) -> None\u001B[39;00m\n\u001B[0;32m--> 506\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/promise/promise.py:502\u001B[0m, in \u001B[0;36mPromise.wait\u001B[0;34m(cls, promise, timeout)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwait\u001B[39m(\u001B[38;5;28mcls\u001B[39m, promise, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;66;03m# type: (Promise, Optional[float]) -> None\u001B[39;00m\n\u001B[0;32m--> 502\u001B[0m     \u001B[43masync_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpromise\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/promise/async_.py:117\u001B[0m, in \u001B[0;36mAsync.wait\u001B[0;34m(self, promise, timeout)\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m promise\u001B[38;5;241m.\u001B[39mis_pending:\n\u001B[1;32m    114\u001B[0m         \u001B[38;5;66;03m# We return if the promise is already\u001B[39;00m\n\u001B[1;32m    115\u001B[0m         \u001B[38;5;66;03m# fulfilled or rejected\u001B[39;00m\n\u001B[1;32m    116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m \u001B[43mtarget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/promise/schedulers/immediate.py:25\u001B[0m, in \u001B[0;36mImmediateScheduler.wait\u001B[0;34m(self, promise, timeout)\u001B[0m\n\u001B[1;32m     22\u001B[0m     e\u001B[38;5;241m.\u001B[39mset()\n\u001B[1;32m     24\u001B[0m promise\u001B[38;5;241m.\u001B[39m_then(on_resolve_or_reject, on_resolve_or_reject)\n\u001B[0;32m---> 25\u001B[0m waited \u001B[38;5;241m=\u001B[39m \u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m waited:\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/usr/lib/python3.9/threading.py:581\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    579\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 581\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/usr/lib/python3.9/threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "data, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split the data into traingin and testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, test_data = data['train'], data['test']\n",
    "print(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_review(dataset):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "\n",
    "    for review, label in dataset:\n",
    "        reviews.append(review.numpy().decode('utf8'))\n",
    "        labels.append(label.numpy())\n",
    "    return reviews, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_review, train_label = parse_review(train_data)\n",
    "test_review, test_label = parse_review(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(train_review), len(train_label))\n",
    "print(len(test_review), len(test_label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize words and padthem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numwords = 20000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=numwords, oov_token='<OOV>')\n",
    "pad = tf.keras.preprocessing.sequence.pad_sequences\n",
    "\n",
    "tokenizer.fit_on_texts(train_review)\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_review)\n",
    "test_seq = tokenizer.texts_to_sequences(test_review)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mx_len = 0\n",
    "for i in train_seq:\n",
    "    mx_len = max(mx_len, len(i))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_pad = pad(train_seq, padding='post', maxlen=120, truncating='post')\n",
    "test_pad = pad(test_seq, padding='post', maxlen=120, truncating='post')\n",
    "\n",
    "print(train_pad.shape)\n",
    "print(test_pad.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.array(test_label).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=numwords+1, output_dim=10, input_length=120),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optim = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "model.compile(\n",
    "    loss = loss,\n",
    "    optimizer=optim,\n",
    "    metrics=['acc']\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_label_mdl = np.array(train_label,dtype=np.float32)\n",
    "test_label_mdl = np.array(test_label,dtype=np.float32)\n",
    "history = model.fit(\n",
    "    train_pad,\n",
    "    train_label_mdl,\n",
    "    epochs=15,\n",
    "    batch_size=250,\n",
    "    validation_data=(test_pad, test_label_mdl)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWTBfNLK3itW",
    "outputId": "f308873d-a640-42d9-d4bf-3f7d5b9b1ac4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 120, 10)           200010    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 16)                1216      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 102       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201335 (786.46 KB)\n",
      "Trainable params: 201335 (786.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDt79OZg3itW",
    "outputId": "d055490d-aa04-4443-ecdc-210df1c7ddbc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 12:21:08.361618: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-01 12:21:08.586596: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-01 12:21:08.603984: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-01 12:21:08.829975: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-01 12:21:08.854500: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.6930 - acc: 0.5079"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 12:21:14.327851: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-01 12:21:14.424194: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-01 12:21:14.437070: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 8s 66ms/step - loss: 0.6930 - acc: 0.5079 - val_loss: 0.6928 - val_acc: 0.5115\n",
      "Epoch 2/15\n",
      "100/100 [==============================] - 5s 52ms/step - loss: 0.6923 - acc: 0.5345 - val_loss: 0.6920 - val_acc: 0.5265\n",
      "Epoch 3/15\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 0.6904 - acc: 0.5551 - val_loss: 0.6897 - val_acc: 0.5717\n",
      "Epoch 4/15\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.6846 - acc: 0.6151 - val_loss: 0.6801 - val_acc: 0.6121\n",
      "Epoch 5/15\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.6072 - acc: 0.7163 - val_loss: 0.5296 - val_acc: 0.7633\n",
      "Epoch 6/15\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 0.4396 - acc: 0.8255 - val_loss: 0.4538 - val_acc: 0.8015\n",
      "Epoch 7/15\n",
      "100/100 [==============================] - 5s 45ms/step - loss: 0.3629 - acc: 0.8653 - val_loss: 0.4323 - val_acc: 0.8113\n",
      "Epoch 8/15\n",
      "100/100 [==============================] - 5s 52ms/step - loss: 0.3144 - acc: 0.8870 - val_loss: 0.4333 - val_acc: 0.8135\n",
      "Epoch 9/15\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.2791 - acc: 0.9022 - val_loss: 0.4261 - val_acc: 0.8177\n",
      "Epoch 10/15\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 0.2506 - acc: 0.9142 - val_loss: 0.4312 - val_acc: 0.8182\n",
      "Epoch 11/15\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.2268 - acc: 0.9225 - val_loss: 0.4405 - val_acc: 0.8162\n",
      "Epoch 12/15\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.2074 - acc: 0.9294 - val_loss: 0.4578 - val_acc: 0.8126\n",
      "Epoch 13/15\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.1921 - acc: 0.9346 - val_loss: 0.4536 - val_acc: 0.8169\n",
      "Epoch 14/15\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 0.1788 - acc: 0.9394 - val_loss: 0.4693 - val_acc: 0.8136\n",
      "Epoch 15/15\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.1674 - acc: 0.9436 - val_loss: 0.4779 - val_acc: 0.8138\n"
     ]
    }
   ],
   "source": [
    "train_label_mdl = np.array(train_label,dtype=np.float32)\n",
    "test_label_mdl = np.array(test_label,dtype=np.float32)\n",
    "history = model.fit(\n",
    "    train_pad,\n",
    "    train_label_mdl,\n",
    "    epochs=15,\n",
    "    batch_size=250,\n",
    "    validation_data=(test_pad, test_label_mdl)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J6CD9Pn3itX",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvLmt2ip3itX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}